{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "directory = os.getcwd()+'/wdir/deptree.txt.v2.'\n",
    "corpus_list = ['training', 'tuning', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n1. ID: Word index, integer starting at 1 for each new sentence; may be a range for tokens with multiple words.\\n2. FORM: Word form or punctuation symbol.\\n3. LEMMA: Lemma or stem of word form.\\n4. UPOSTAG: Universal part-of-speech tag drawn from our revised version of the Google universal POS tags.\\n5. XPOSTAG: Language-specific part-of-speech tag; underscore if not available.\\n6. FEATS: List of morphological features from the universal feature inventory or from a defined language-specific extension; underscore if not available.\\n7. HEAD: Head of the current token, which is either a value of ID or zero (0).\\n8. DEPREL: Universal Stanford dependency relation to the HEAD (root iff HEAD = 0) or a defined language-specific subtype of one.\\n9. DEPS: List of secondary dependencies (head-deprel pairs).\\n10. MISC: Any other annotation.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "1. ID: Word index, integer starting at 1 for each new sentence; may be a range for tokens with multiple words.\n",
    "2. FORM: Word form or punctuation symbol.\n",
    "3. LEMMA: Lemma or stem of word form.\n",
    "4. UPOSTAG: Universal part-of-speech tag drawn from our revised version of the Google universal POS tags.\n",
    "5. XPOSTAG: Language-specific part-of-speech tag; underscore if not available.\n",
    "6. FEATS: List of morphological features from the universal feature inventory or from a defined language-specific extension; underscore if not available.\n",
    "7. HEAD: Head of the current token, which is either a value of ID or zero (0).\n",
    "8. DEPREL: Universal Stanford dependency relation to the HEAD (root iff HEAD = 0) or a defined language-specific subtype of one.\n",
    "9. DEPS: List of secondary dependencies (head-deprel pairs).\n",
    "10. MISC: Any other annotation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jmin/workspace/syntaxnet_easy_sejong/models_sejong_conll/sejong_training converted!\n",
      "/home/jmin/workspace/syntaxnet_easy_sejong/models_sejong_conll/sejong_tuning converted!\n",
      "/home/jmin/workspace/syntaxnet_easy_sejong/models_sejong_conll/sejong_test converted!\n"
     ]
    }
   ],
   "source": [
    "for corpus_type in corpus_list:\n",
    "    corpus = directory + corpus_type\n",
    "    with codecs.open(corpus, 'r') as readfile:\n",
    "        OUT_FILENAME = os.path.abspath(os.path.join(os.getcwd(), os.pardir)) + '/models_sejong_conll/sejong_' + corpus_type\n",
    "\n",
    "        sentence_buffer = \"\"\n",
    "        error_sentence = False\n",
    "        # print_dict = {'ID', 'FORM', 'LEMMA', 'UPOSTAG', 'XPOSTAG', 'FEATS', 'HEAD', 'DEPREL', 'DEPS', 'MISC'}\n",
    "        with codecs.open(OUT_FILENAME, 'w') as writefile:\n",
    "            line_counter = 1\n",
    "            \n",
    "            for line in readfile:\n",
    "                if line == '\\n':\n",
    "                    if error_sentence == False:\n",
    "                        sentence_buffer += '\\n'\n",
    "                        writefile.write(sentence_buffer)\n",
    "                    \n",
    "                    error_sentence = False\n",
    "                    sentence_buffer = \"\"\n",
    "\n",
    "                \n",
    "                else:\n",
    "                    bucket = line.split('\\t')\n",
    "                    \"\"\"\n",
    "                    0: ID\n",
    "                    1: FORM\n",
    "                    2: LEMMA + POS\n",
    "                    3. DEPREL\n",
    "                    4. HEAD\n",
    "                    \"\"\"\n",
    "                    print_dict = dict()\n",
    "                    POS = []\n",
    "                    Lemma = []\n",
    "                    #print(bucket)\n",
    "                    \n",
    "                    POS_Lemma = re.split('[ +]+', bucket[2])\n",
    "                    #print(\"POS_Lemma: \"+str(POS_Lemma))\n",
    "                    # print(line_counter, POS_Lemma)\n",
    "                    \n",
    "                    POS_match = []\n",
    "                    Lemma_match = []\n",
    "                    for POS_Lemma_x in POS_Lemma:\n",
    "                        #print(\"POS_Lemma_x: \", POS_Lemma_x)\n",
    "                        \n",
    "                        try:\n",
    "                            matched_POS = re.findall('(\\/[A-Z_]+)', POS_Lemma_x)[0][1:]\n",
    "                            #print(\"matched_POS: \", matched_POS)\n",
    "                        except IndexError as e:\n",
    "                            print(e, \"\\n\", \"Line \", line_counter, \" contains errors at\")\n",
    "                            print(POS_Lemma, line)\n",
    "                            error_sentence = True\n",
    "\n",
    "\n",
    "                        matched_Lemma = re.sub(matched_POS, '', POS_Lemma_x)[:-1]\n",
    "                        #print(\"matched_Lemma:\", matched_Lemma)\n",
    "                        \n",
    "                        POS_match.append(matched_POS)\n",
    "                        Lemma_match.append(matched_Lemma)\n",
    "                    \n",
    "                    #print(\"POS_match: \"+str(POS_match))\n",
    "                    #print(\"Lemma_match: \" + str(Lemma_match))\n",
    "                    \n",
    "\n",
    "                    POS = '+'.join(POS_match)\n",
    "                    #print(\"POS: \"+POS)\n",
    "                    \n",
    "                    #Lemma = re.findall('(.*)(\\/[A-Z_]+)', bucket[2])\n",
    "                    #print(Lemma.groups())\n",
    "                    \n",
    "                    # Lemma_match = re.findall('([]\\/)')\n",
    "\n",
    "\n",
    "                    print_dict['ID'] = bucket[0]\n",
    "                    print_dict['FORM'] = bucket[1]\n",
    "                    print_dict['LEMMA'] = Lemma_match[0]\n",
    "                    print_dict['UPOSTAG'] = POS_match[0]\n",
    "                    print_dict['XPOSTAG'] = POS\n",
    "                    print_dict['FEATS'] = '_'\n",
    "                    print_dict['HEAD'] = bucket[4][:-1]\n",
    "                    print_dict['DEPREL'] = bucket[3]\n",
    "                    print_dict['DEPS'] = '_'\n",
    "                    print_dict['MISC'] = '_'\n",
    "                    \n",
    "                    line_to_write = str(print_dict['ID']) + '\\t' + str(print_dict['FORM']) + \\\n",
    "                                    '\\t' + str(print_dict['LEMMA']) + '\\t' + str(print_dict['UPOSTAG']) + \\\n",
    "                                    '\\t' + str(print_dict['XPOSTAG']) + '\\t' + str(print_dict['FEATS']) + \\\n",
    "                                    '\\t' + str(print_dict['HEAD']) + '\\t' + str(print_dict['DEPREL']) + \\\n",
    "                                    '\\t' + str(print_dict['DEPS']) + '\\t' + str(print_dict['MISC']) + '\\n'\n",
    "                            \n",
    "                    #print(line_to_write)\n",
    "                    #break\n",
    "                    \n",
    "                    sentence_buffer += line_to_write\n",
    "                line_counter += 1\n",
    "            print(str(OUT_FILENAME)+ \" converted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Sejong Corpora have been converted!\n"
     ]
    }
   ],
   "source": [
    "print(\"All Sejong Corpora have been converted!\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda_two]",
   "language": "python",
   "name": "Python [conda_two]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
